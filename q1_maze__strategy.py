# -*- coding: utf-8 -*-
"""Q1_Maze _Strategy

Automatically generated by Colaboratory.
Author: Xiaomeng Yao

The strategy consists of two parts. The first part is to utilize the imaginary startful API Maze and a recursion algorithm to 
find the transition information of the maze, stored in a stack. The stack is then transferred to a reward matrix called R matrix. 

In the second part, the R matrix will be consumed by a Q_learn reinforcement learning agent to find the shortest path to room 1, 
starting from any room (which will include room 0 as a special case).  


Usage: 

run TraverseMaze([room_transition(0)],0) to generate 'stack' from the initial point

run convertStack(stack) to generate the reward matrix R

run robot=QLearnAgent(R,0).train() to train the robot

run robot.shortest_path() to find the shortest path from room 0 to room 1


Warning: the script has not been tested as the stateful API is not available
"""

import numpy as np
# The imaginary API 
import Maze as Maze

# The rest of code is to generate the R matrix by using the stateful API called Maze and the recusion function. ###
 
 
def room_transition(room_number)
    """
    Find the current room's transtion state via the API.
    
    """
    #set the first entry to be the original room
    trans=[room_number]
    colors=Maze.get_exists()
    # prevent going through the black door when staying in room 0
    if room_number == 0:
       colors.remove('black')

    for i in range(len(colors)):
    # find the number of the connected room.
        trans.append(Maze.go_through_door(colors[i])) 
    #return to the original room.
    #ensure the state won't be changed after the search.
        Maze.go_through_door(colors[i])
    return trans   



def TraverseMaze(room,stack):
    """
    Use a simple recursion algo to reach room 1,
    and return all the intermediate transition information.
    Note that this is not an efficient algorithm. 
    Shortest Path cannot be found via this approach either.
    """
    global stack 
    
    if room==1:
       break
    else:
        for i in range(len(room_transition(room))-1)
            colors=Maze.get_exists()
            room_next=Maze.go_through_door(colors[i])
            stack.append(room_transition(room_next))
            TraverseMaze(room_next)
       
def ConvertStack(stack):
    """
    Convert the stack to the reward matrix for reinforement learning. 
    """
    # make the stack only contain unique elements.
    stack=list(set(stack))
    R=np.matrix(np.negative(np.ones([len(stack),len(stack)])))
    # reward the transition to room 1 as 100
    for k in range(len(stack[0])-1):
        R[0,stack[0][k]]=100
    # reward the transitions to other rooms as 0
    for i in range(len(stack)-1):
        for j in range(len(stack[i])-1):
            R[i,stack[i][j]]=0
    return R

#### The Q learning agent. Note that the epoch is set as 100000 and the gamma is fixed as 0.8. These two values can be easily customised if required.####


class QLearnAgent():
    
      
    def __init__(self,R,start_room):
        self.start_room=start_room
        self.Q=None
        self.R=R
        self.intermediate_Q_mat_at_step=1000
        self.gamma=0.8
        self.epochs=100000
   
   # public members
    def train(self):
        """
        Training the Q_agent. 
        """
        self.__initialise_Q()

        for i in range(self.epochs): 
            state_1 = np.random.randint(0, int(self.Q.shape[0]))
            available_act_1 = self. __available_actions(state_1)
            action_1 = self. __sample_next_action(available_act_1)
            self.__update(state_1,action_1)
            if((i % self.intermediate_Q_mat_at_step) == 0):
                print("Q-matrix at step ", i)
                print(self.Q/np.max(self.Q)*100)
            

    def shortest_path(self):
        """
        Shortest path finder.
        After calling train(), use this funciton to find the shortest path
        """
        current_state=self.start_room
        #room 0 is always at the end of the R_matrix by recursion.
        goal_state=self.R.shape[0]-1
        steps=[current_state]
        while current_state != goal_state:
            next_step_index = np.where(self.Q[current_state,] == np.max(self.Q[current_state,]))[1]
    
            if next_step_index.shape[0] > 1:
                next_step_index = int(np.random.choice(next_step_index, size = 1))
            else:
                next_step_index = int(next_step_index)
        
            steps.append(next_step_index)
            current_state = next_step_index

        # print the shortest path selected 
        print("shortest path:")
        print(steps)
   
    # private members
    def __initialise_Q(self):
        """
        Initialsie Q based on R.
        """
        self.Q=np.matrix(np.zeros([self.R.shape[0],self.R.shape[0]]))


    def __sample_next_action(self,available_act):
        """
        Randomly sample the next action.
        """
        next_action = int(np.random.choice(available_act,1))
        return next_action


    def __available_actions(self,state):
        current_state_row = self.R[state,]
        av_act = np.where(current_state_row >= 0)[1]
        return av_act


    def __update(self,current_state, action):
        """
        Update the Q-matrix for training phase.
        """
        max_index = np.where(self.Q[action,] == np.max(self.Q[action,]))[1]
        if max_index.shape[0] > 1:
              max_index = int(np.random.choice(max_index, size = 1))
        else:
              max_index = int(max_index)  
        # Q learning formula
        self.Q[current_state, action] = self.R[current_state, action] + self.gamma*self.Q[action, max_index]